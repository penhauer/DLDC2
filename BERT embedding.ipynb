{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel, GPT2Tokenizer, GPT2Model\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tqdm import tqdm\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load and preprocess data\n",
    "def load_data(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "    texts = data['text'].tolist()\n",
    "    scores = data['score'].tolist()\n",
    "    return texts, scores\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the MLP model\n",
    "class MLPModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPModel, self).__init__()\n",
    "        self.l1 = nn.Sequential(\n",
    "            \n",
    "            # BERT 10\n",
    "            nn.Linear(9984, 4096),\n",
    "            # nn.Dropout(0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4096, 1024),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Linear(2048, 1024),\n",
    "            # nn.ReLU(),\n",
    "            nn.Linear(1024, 256),\n",
    "            # nn.Dropout(0.2),\n",
    "            nn.ReLU(),\n",
    "            # nn.Linear(512, 256),   \n",
    "            # nn.ReLU(),\n",
    "            nn.Linear(256, 128),   \n",
    "            # nn.Dropout(0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 32),\n",
    "            # nn.Dropout(0.2),\n",
    "            nn.ReLU(),\n",
    "            # nn.Linear(64, 32),\n",
    "            # nn.ReLU(),\n",
    "            nn.Linear(32, 1),\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.l1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_on_validation(val_loader, model):\n",
    "    model.eval()\n",
    "    val_preds = []\n",
    "    val_labels = []\n",
    "    with torch.no_grad():\n",
    "        # for inputs, labels in tqdm(val_loader, desc=f\"Epoch {epoch + 1}/{num_epochs} - Validation\"):\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            val_preds.extend(outputs.cpu().numpy())\n",
    "            val_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "    val_rmse = mean_squared_error(val_labels, val_preds, squared=False)\n",
    "    return val_rmse\n",
    "\n",
    "\n",
    "# Train MLP model\n",
    "def train_mlp_model(train_loader, val_loader, model, criterion, optimizer, num_epochs=5, patience=10):\n",
    "    model = model.to(device)\n",
    "\n",
    "\n",
    "    best_val_rmse = float('inf')\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        model.train()\n",
    "        # for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs} - Training\"):\n",
    "        train_preds = []\n",
    "        train_labels = []\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            loss = criterion(outputs, labels.unsqueeze(1).float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        \n",
    "            train_preds.extend(outputs.detach().cpu().numpy())\n",
    "            train_labels.extend(labels.detach().cpu().numpy())\n",
    "\n",
    "        train_rmse = mean_squared_error(train_labels, train_preds, squared=False)\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Test RMSE: {train_rmse:.4f}\")\n",
    "\n",
    "\n",
    "        val_rmse = run_on_validation(val_loader, model)\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Validation RMSE: {val_rmse:.4f}\")\n",
    "\n",
    "        if val_rmse < best_val_rmse:\n",
    "            best_val_rmse = val_rmse\n",
    "            no_improvement = 0\n",
    "        else:\n",
    "            no_improvement += 1\n",
    "            if no_improvement >= patience:\n",
    "                print(f\"No improvement for {patience} consecutive epochs. Early stopping.\")\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModel\n",
    "# import torch\n",
    "# import numpy as np\n",
    "\n",
    "# def get_embeddings(texts, model_name):\n",
    "#     # Load pre-trained model and tokenizer\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(model_name, model_max_length=514)\n",
    "#     model = AutoModel.from_pretrained(\n",
    "#         model_name,\n",
    "#     ).to(device)\n",
    "\n",
    "#     embeddings = []\n",
    "\n",
    "#     for text in tqdm(texts):\n",
    "#         # Tokenize input text and obtain embeddings\n",
    "#         inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "#         with torch.no_grad():\n",
    "#             outputs = model(**inputs)\n",
    "#         # Extract the embeddings from the last layer\n",
    "#         last_hidden_states = outputs.last_hidden_state\n",
    "#         # Average pooling to get a fixed-size embedding for the entire text\n",
    "#         avg_pooling = torch.mean(last_hidden_states, dim=1)\n",
    "#         # Convert tensor to numpy array\n",
    "#         avg_pooling = avg_pooling.cpu().numpy()\n",
    "#         # Append the embedding to the list\n",
    "#         embeddings.append(avg_pooling)\n",
    "\n",
    "#     return np.concatenate(embeddings, axis=0)\n",
    "\n",
    "# models = [\n",
    "#     'xlm-mlm-enfr-1024',\n",
    "#     'distilbert-base-cased',\n",
    "#     'bert-base-uncased',\n",
    "#     'roberta-base',\n",
    "    \n",
    "    \n",
    "#     'bert-base-cased',\n",
    "#     'xlm-roberta-base',\n",
    "#     'xlnet-base-cased',    \n",
    "\n",
    "# # 'cardiffnlp/twitter-roberta-base-sentiment',\n",
    "# #     'ctrl',\n",
    "# #     'transfo-xl-wt103',\n",
    "# #     'openai-gpt',\n",
    "# #     'gpt2'\n",
    "# ]\n",
    "\n",
    "\n",
    "# def get_all_embeddings(texts):\n",
    "#     all_embeddings = []\n",
    "\n",
    "#     for model_name in models:\n",
    "#         print(f\"calculating embeddings for {model_name}\")\n",
    "#         embeddings = get_embeddings(texts, model_name)\n",
    "#         all_embeddings.append(embeddings)\n",
    "\n",
    "#     return np.concatenate(all_embeddings, axis=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and create BERT embeddings\n",
    "def create_gpt2_embeddings(texts):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model = GPT2Model.from_pretrained('gpt2')\n",
    "    model = model.to(device)\n",
    "\n",
    "    embeddings = []\n",
    "    for text in tqdm(texts, desc=\"Creating GPT-2 Embeddings\"):\n",
    "        inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n",
    "        inputs = inputs.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            last_hidden_states = outputs.last_hidden_state\n",
    "\n",
    "        # Use the mean of the last hidden states as the embedding\n",
    "        embedding = torch.mean(last_hidden_states, dim=1).squeeze().detach().cpu().numpy()\n",
    "        embeddings.append(embedding)\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "\n",
    "def create_bert_embeddings(texts):\n",
    "    bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    bert_model = BertModel.from_pretrained(\n",
    "        'bert-base-uncased',\n",
    "        output_hidden_states = True,\n",
    "    ).to(device)\n",
    "\n",
    "    \n",
    "    embeddings = []\n",
    "    num_layers = 13\n",
    "    max_length = 512\n",
    "    for text in tqdm(texts, desc=\"Creating BERT Embeddings\"):\n",
    "        head = bert_tokenizer(text, return_tensors='pt', truncation=True, padding=True).to(device)\n",
    "        \n",
    "        ind = max(len(text.split(' ')) - 2 * max_length, 0)\n",
    "        tail = bert_tokenizer(text[ind:], return_tensors='pt', truncation=True, padding=True).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output_head = bert_model(**head)\n",
    "            output_tail = bert_model(**tail)\n",
    "        \n",
    "            \n",
    "        embedding_head = torch.cat([output_head.hidden_states[i] for i in range(num_layers)], dim=-1)\n",
    "        embedding_tail = torch.cat([output_tail.hidden_states[i] for i in range(num_layers)], dim=-1)    \n",
    "        \n",
    "        mean_embedding = (torch.mean(embedding_head, dim=1) + torch.mean(embedding_tail, dim=1)) / 2\n",
    "        # mean_embedding = (torch.mean(embedding_head, dim=1)\n",
    "                          \n",
    "        final_embedding = mean_embedding.squeeze().detach().cpu().numpy()\n",
    "        embeddings.append(final_embedding)\n",
    "\n",
    "    return embeddings\n",
    "    \n",
    "\n",
    "    # max_length=512\n",
    "    # num_layers = 13\n",
    "    # embeddings = []\n",
    "    # counter = 0\n",
    "    # for text in tqdm(texts, desc=\"Creating BERT Embeddings\"):\n",
    "    #     counter += 1\n",
    "    #     if counter >= 100:\n",
    "    #         gc.collect()\n",
    "    #         counter = 0\n",
    "    #         libc = ctypes.CDLL(\"libc.so.6\") # clearing cache \n",
    "    #         libc.malloc_trim(0)\n",
    "        \n",
    "    #     inputs = bert_tokenizer(text, return_tensors='pt', truncation=True, padding=True).to(device)\n",
    "    #     input_ids = inputs['input_ids']\n",
    "    #     attention_mask = inputs['attention_mask']\n",
    "\n",
    "    #     # Determine the length of the text\n",
    "    #     text_length = len(input_ids[0])\n",
    "\n",
    "    #     # Determine the length of the head and tail for each text\n",
    "    #     head_length = min(max_length, text_length)\n",
    "    #     # tail_length = max_length - head_length\n",
    "    #     tail_length = head_length\n",
    "\n",
    "    #     # Extract the head and tail embeddings\n",
    "    #     head_inputs = {'input_ids': input_ids[:, :head_length], 'attention_mask': attention_mask[:, :head_length]}\n",
    "    #     tail_inputs = {'input_ids': input_ids[:, -tail_length:], 'attention_mask': attention_mask[:, -tail_length:]}\n",
    "\n",
    "    #     # head_inputs = {key: value.to(device) for key, value in head_inputs.items()}\n",
    "    #     # tail_inputs = {key: value.to(device) for key, value in tail_inputs.items()}\n",
    "\n",
    "    #     assert head_inputs['input_ids'].size()[1] == head_length\n",
    "    #     assert tail_inputs['input_ids'].size()[1] == head_length\n",
    "    #     assert head_inputs['attention_mask'].size()[1] == head_length\n",
    "    #     assert tail_inputs['attention_mask'].size()[1] == head_length\n",
    "\n",
    "\n",
    "    #     with torch.no_grad():\n",
    "    #         head_outputs = bert_model(**head_inputs)\n",
    "    #         tail_outputs = bert_model(**tail_inputs)\n",
    "\n",
    "    #         # head_hidden_states = torch.stack(head_outputs.hidden_states[-num_layers:])\n",
    "    #         # tail_hidden_states = torch.stack(tail_outputs.hidden_states[-num_layers:])\n",
    "            \n",
    "    #         h_e = torch.cat([head_outputs.hidden_states[i] for i in range(num_layers)], dim=-1)\n",
    "    #         t_e = torch.cat([tail_outputs.hidden_states[i] for i in range(num_layers)], dim=-1)\n",
    "\n",
    "                \n",
    "    #         # Average the embeddings for the head and tail\n",
    "    #         final_embedding = (torch.mean(h_e, dim=1) + torch.mean(t_e, dim=1)) / 2\n",
    "\n",
    "    #         # print(head_outputs.hidden_states[1].size())\n",
    "    #         # print(head_hidden_states.size())\n",
    "    #         print(h_e.size())\n",
    "    #         # print(final_embedding.size())\n",
    "\n",
    "\n",
    "    \n",
    "    #     # del head_hidden_states\n",
    "    #     # del tail_hidden_states\n",
    "    #     # del head_outputs\n",
    "    #     # del tail_outputs\n",
    "    #     # del head_inputs\n",
    "    #     # del tail_inputs\n",
    "    #     # del inputs\n",
    "\n",
    "    #     final_embedding = final_embedding.squeeze().detach().cpu().numpy()\n",
    "    #     embeddings.append(final_embedding)\n",
    "\n",
    "    # return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_keywords(text):\n",
    "    words = text.split()\n",
    "\n",
    "    for i in range(len(words)):\n",
    "        if words[i].startswith('@ORGANIZATION'):\n",
    "            words[i] = 'organization'\n",
    "        elif words[i].startswith('@TIME'):\n",
    "            words[i] = '2PM'\n",
    "        elif words[i].startswith('@NUM'):\n",
    "            words[i] = '5'\n",
    "        elif words[i].startswith('@MONEY'):\n",
    "            words[i] = 'dollar'\n",
    "        elif words[i].startswith('@PERCENT'):\n",
    "            words[i] = '50%'\n",
    "        elif words[i].startswith('@LOCATION'):\n",
    "            words[i] = 'Waterloo'\n",
    "        elif words[i].startswith('@DATE'):\n",
    "            words[i] = 'November 17'\n",
    "        elif words[i].startswith('@CITY'):\n",
    "            words[i] = 'Waterloo'\n",
    "\n",
    "    return ' '.join(words)\n",
    "\n",
    "\n",
    "def replace_keyword_list(texts):\n",
    "    return list(map(replace_keywords, texts))\n",
    "\n",
    "\n",
    "def embedding_after_replacement(texts):\n",
    "    return create_bert_embeddings(replace_keyword_list(texts))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating BERT Embeddings: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 2400/2400 [01:29<00:00, 26.84it/s]\n",
      "Creating BERT Embeddings: 100%|█████████████████████████████████████████████████████████████████████████████████████████████| 600/600 [00:22<00:00, 26.55it/s]\n"
     ]
    }
   ],
   "source": [
    "train_texts, train_scores = load_data('train.csv')\n",
    "train_texts, val_texts, train_scores, val_scores = train_test_split(\n",
    "    train_texts, train_scores, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "embedding_function = embedding_after_replacement\n",
    "train_embeddings = embedding_function(train_texts)\n",
    "val_embeddings = embedding_function(val_texts)\n",
    "\n",
    "train_data = TensorDataset(torch.Tensor(train_embeddings), torch.tensor(train_scores))\n",
    "val_data = TensorDataset(torch.Tensor(val_embeddings), torch.tensor(val_scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|█▌                                                                                                                       | 2/150 [00:00<00:27,  5.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150, Test RMSE: 3.4192\n",
      "Epoch 1/150, Validation RMSE: 3.3077\n",
      "Epoch 2/150, Test RMSE: 3.1845\n",
      "Epoch 2/150, Validation RMSE: 2.9113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|███▏                                                                                                                     | 4/150 [00:00<00:26,  5.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/150, Test RMSE: 2.5843\n",
      "Epoch 3/150, Validation RMSE: 1.9717\n",
      "Epoch 4/150, Test RMSE: 1.4387\n",
      "Epoch 4/150, Validation RMSE: 0.6794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|████▊                                                                                                                    | 6/150 [00:01<00:26,  5.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/150, Test RMSE: 0.7600\n",
      "Epoch 5/150, Validation RMSE: 0.8919\n",
      "Epoch 6/150, Test RMSE: 0.7055\n",
      "Epoch 6/150, Validation RMSE: 0.6300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|██████▍                                                                                                                  | 8/150 [00:01<00:25,  5.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/150, Test RMSE: 0.6227\n",
      "Epoch 7/150, Validation RMSE: 0.6295\n",
      "Epoch 8/150, Test RMSE: 0.5741\n",
      "Epoch 8/150, Validation RMSE: 0.5901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|████████                                                                                                                | 10/150 [00:01<00:25,  5.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/150, Test RMSE: 0.5552\n",
      "Epoch 9/150, Validation RMSE: 0.5668\n",
      "Epoch 10/150, Test RMSE: 0.5339\n",
      "Epoch 10/150, Validation RMSE: 0.5483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|█████████▌                                                                                                              | 12/150 [00:02<00:24,  5.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/150, Test RMSE: 0.5174\n",
      "Epoch 11/150, Validation RMSE: 0.5273\n",
      "Epoch 12/150, Test RMSE: 0.4991\n",
      "Epoch 12/150, Validation RMSE: 0.5064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|███████████▏                                                                                                            | 14/150 [00:02<00:24,  5.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/150, Test RMSE: 0.4814\n",
      "Epoch 13/150, Validation RMSE: 0.4842\n",
      "Epoch 14/150, Test RMSE: 0.4621\n",
      "Epoch 14/150, Validation RMSE: 0.4615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|████████████▊                                                                                                           | 16/150 [00:02<00:24,  5.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/150, Test RMSE: 0.4438\n",
      "Epoch 15/150, Validation RMSE: 0.4394\n",
      "Epoch 16/150, Test RMSE: 0.4268\n",
      "Epoch 16/150, Validation RMSE: 0.4185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|██████████████▍                                                                                                         | 18/150 [00:03<00:23,  5.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/150, Test RMSE: 0.4113\n",
      "Epoch 17/150, Validation RMSE: 0.4026\n",
      "Epoch 18/150, Test RMSE: 0.3999\n",
      "Epoch 18/150, Validation RMSE: 0.3875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|████████████████                                                                                                        | 20/150 [00:03<00:23,  5.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/150, Test RMSE: 0.3907\n",
      "Epoch 19/150, Validation RMSE: 0.3776\n",
      "Epoch 20/150, Test RMSE: 0.3862\n",
      "Epoch 20/150, Validation RMSE: 0.3728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█████████████████▌                                                                                                      | 22/150 [00:03<00:23,  5.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/150, Test RMSE: 0.3818\n",
      "Epoch 21/150, Validation RMSE: 0.3672\n",
      "Epoch 22/150, Test RMSE: 0.3799\n",
      "Epoch 22/150, Validation RMSE: 0.3649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|███████████████████▏                                                                                                    | 24/150 [00:04<00:22,  5.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/150, Test RMSE: 0.3777\n",
      "Epoch 23/150, Validation RMSE: 0.3624\n",
      "Epoch 24/150, Test RMSE: 0.3765\n",
      "Epoch 24/150, Validation RMSE: 0.3609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|████████████████████▊                                                                                                   | 26/150 [00:04<00:22,  5.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/150, Test RMSE: 0.3752\n",
      "Epoch 25/150, Validation RMSE: 0.3594\n",
      "Epoch 26/150, Test RMSE: 0.3740\n",
      "Epoch 26/150, Validation RMSE: 0.3583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|██████████████████████▍                                                                                                 | 28/150 [00:05<00:22,  5.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/150, Test RMSE: 0.3729\n",
      "Epoch 27/150, Validation RMSE: 0.3581\n",
      "Epoch 28/150, Test RMSE: 0.3724\n",
      "Epoch 28/150, Validation RMSE: 0.3572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████████████████████                                                                                                | 30/150 [00:05<00:21,  5.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/150, Test RMSE: 0.3715\n",
      "Epoch 29/150, Validation RMSE: 0.3567\n",
      "Epoch 30/150, Test RMSE: 0.3702\n",
      "Epoch 30/150, Validation RMSE: 0.3563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|█████████████████████████▌                                                                                              | 32/150 [00:05<00:21,  5.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/150, Test RMSE: 0.3685\n",
      "Epoch 31/150, Validation RMSE: 0.3548\n",
      "Epoch 32/150, Test RMSE: 0.3674\n",
      "Epoch 32/150, Validation RMSE: 0.3536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|███████████████████████████▏                                                                                            | 34/150 [00:06<00:20,  5.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/150, Test RMSE: 0.3667\n",
      "Epoch 33/150, Validation RMSE: 0.3530\n",
      "Epoch 34/150, Test RMSE: 0.3664\n",
      "Epoch 34/150, Validation RMSE: 0.3545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|████████████████████████████▊                                                                                           | 36/150 [00:06<00:20,  5.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/150, Test RMSE: 0.3645\n",
      "Epoch 35/150, Validation RMSE: 0.3525\n",
      "Epoch 36/150, Test RMSE: 0.3642\n",
      "Epoch 36/150, Validation RMSE: 0.3519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██████████████████████████████▍                                                                                         | 38/150 [00:06<00:20,  5.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/150, Test RMSE: 0.3631\n",
      "Epoch 37/150, Validation RMSE: 0.3518\n",
      "Epoch 38/150, Test RMSE: 0.3633\n",
      "Epoch 38/150, Validation RMSE: 0.3498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|████████████████████████████████                                                                                        | 40/150 [00:07<00:19,  5.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/150, Test RMSE: 0.3619\n",
      "Epoch 39/150, Validation RMSE: 0.3499\n",
      "Epoch 40/150, Test RMSE: 0.3605\n",
      "Epoch 40/150, Validation RMSE: 0.3489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|█████████████████████████████████▌                                                                                      | 42/150 [00:07<00:19,  5.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/150, Test RMSE: 0.3590\n",
      "Epoch 41/150, Validation RMSE: 0.3492\n",
      "Epoch 42/150, Test RMSE: 0.3586\n",
      "Epoch 42/150, Validation RMSE: 0.3480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|███████████████████████████████████▏                                                                                    | 44/150 [00:07<00:19,  5.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/150, Test RMSE: 0.3577\n",
      "Epoch 43/150, Validation RMSE: 0.3506\n",
      "Epoch 44/150, Test RMSE: 0.3572\n",
      "Epoch 44/150, Validation RMSE: 0.3473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|████████████████████████████████████▊                                                                                   | 46/150 [00:08<00:18,  5.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/150, Test RMSE: 0.3555\n",
      "Epoch 45/150, Validation RMSE: 0.3475\n",
      "Epoch 46/150, Test RMSE: 0.3548\n",
      "Epoch 46/150, Validation RMSE: 0.3465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|██████████████████████████████████████▍                                                                                 | 48/150 [00:08<00:18,  5.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/150, Test RMSE: 0.3537\n",
      "Epoch 47/150, Validation RMSE: 0.3457\n",
      "Epoch 48/150, Test RMSE: 0.3523\n",
      "Epoch 48/150, Validation RMSE: 0.3457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|████████████████████████████████████████                                                                                | 50/150 [00:09<00:18,  5.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/150, Test RMSE: 0.3517\n",
      "Epoch 49/150, Validation RMSE: 0.3481\n",
      "Epoch 50/150, Test RMSE: 0.3514\n",
      "Epoch 50/150, Validation RMSE: 0.3444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|█████████████████████████████████████████▌                                                                              | 52/150 [00:09<00:17,  5.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/150, Test RMSE: 0.3495\n",
      "Epoch 51/150, Validation RMSE: 0.3443\n",
      "Epoch 52/150, Test RMSE: 0.3485\n",
      "Epoch 52/150, Validation RMSE: 0.3438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███████████████████████████████████████████▏                                                                            | 54/150 [00:09<00:17,  5.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/150, Test RMSE: 0.3474\n",
      "Epoch 53/150, Validation RMSE: 0.3431\n",
      "Epoch 54/150, Test RMSE: 0.3467\n",
      "Epoch 54/150, Validation RMSE: 0.3432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|████████████████████████████████████████████▊                                                                           | 56/150 [00:10<00:17,  5.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/150, Test RMSE: 0.3457\n",
      "Epoch 55/150, Validation RMSE: 0.3429\n",
      "Epoch 56/150, Test RMSE: 0.3450\n",
      "Epoch 56/150, Validation RMSE: 0.3421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|██████████████████████████████████████████████▍                                                                         | 58/150 [00:10<00:16,  5.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/150, Test RMSE: 0.3446\n",
      "Epoch 57/150, Validation RMSE: 0.3431\n",
      "Epoch 58/150, Test RMSE: 0.3452\n",
      "Epoch 58/150, Validation RMSE: 0.3437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████████████████████████████████████████████████                                                                        | 60/150 [00:10<00:16,  5.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/150, Test RMSE: 0.3421\n",
      "Epoch 59/150, Validation RMSE: 0.3449\n",
      "Epoch 60/150, Test RMSE: 0.3440\n",
      "Epoch 60/150, Validation RMSE: 0.3412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|█████████████████████████████████████████████████▌                                                                      | 62/150 [00:11<00:15,  5.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/150, Test RMSE: 0.3413\n",
      "Epoch 61/150, Validation RMSE: 0.3450\n",
      "Epoch 62/150, Test RMSE: 0.3410\n",
      "Epoch 62/150, Validation RMSE: 0.3433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|███████████████████████████████████████████████████▏                                                                    | 64/150 [00:11<00:15,  5.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63/150, Test RMSE: 0.3410\n",
      "Epoch 63/150, Validation RMSE: 0.3411\n",
      "Epoch 64/150, Test RMSE: 0.3381\n",
      "Epoch 64/150, Validation RMSE: 0.3408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████████████████████████████████████████████████████▊                                                                   | 66/150 [00:11<00:15,  5.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65/150, Test RMSE: 0.3356\n",
      "Epoch 65/150, Validation RMSE: 0.3427\n",
      "Epoch 66/150, Test RMSE: 0.3365\n",
      "Epoch 66/150, Validation RMSE: 0.3406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|██████████████████████████████████████████████████████▍                                                                 | 68/150 [00:12<00:14,  5.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67/150, Test RMSE: 0.3346\n",
      "Epoch 67/150, Validation RMSE: 0.3402\n",
      "Epoch 68/150, Test RMSE: 0.3338\n",
      "Epoch 68/150, Validation RMSE: 0.3401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████████████████████████████████████████████████████████                                                                | 70/150 [00:12<00:14,  5.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69/150, Test RMSE: 0.3321\n",
      "Epoch 69/150, Validation RMSE: 0.3414\n",
      "Epoch 70/150, Test RMSE: 0.3321\n",
      "Epoch 70/150, Validation RMSE: 0.3393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|█████████████████████████████████████████████████████████▌                                                              | 72/150 [00:13<00:14,  5.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/150, Test RMSE: 0.3309\n",
      "Epoch 71/150, Validation RMSE: 0.3406\n",
      "Epoch 72/150, Test RMSE: 0.3308\n",
      "Epoch 72/150, Validation RMSE: 0.3393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|███████████████████████████████████████████████████████████▏                                                            | 74/150 [00:13<00:13,  5.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/150, Test RMSE: 0.3295\n",
      "Epoch 73/150, Validation RMSE: 0.3387\n",
      "Epoch 74/150, Test RMSE: 0.3285\n",
      "Epoch 74/150, Validation RMSE: 0.3392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|████████████████████████████████████████████████████████████▊                                                           | 76/150 [00:13<00:13,  5.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/150, Test RMSE: 0.3273\n",
      "Epoch 75/150, Validation RMSE: 0.3385\n",
      "Epoch 76/150, Test RMSE: 0.3266\n",
      "Epoch 76/150, Validation RMSE: 0.3393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████████████████████████████████████████████████████████████▌                                                          | 77/150 [00:13<00:13,  5.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/150, Test RMSE: 0.3258\n",
      "Epoch 77/150, Validation RMSE: 0.3393\n",
      "Epoch 78/150, Test RMSE: 0.3249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|███████████████████████████████████████████████████████████████▏                                                        | 79/150 [00:14<00:13,  5.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/150, Validation RMSE: 0.3393\n",
      "Epoch 79/150, Test RMSE: 0.3243\n",
      "Epoch 79/150, Validation RMSE: 0.3406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|███████████████████████████████████████████████████████████████▏                                                        | 79/150 [00:14<00:13,  5.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/150, Test RMSE: 0.3236\n",
      "Epoch 80/150, Validation RMSE: 0.3387\n",
      "No improvement for 5 consecutive epochs. Early stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Create DataLoader\n",
    "patience = 5\n",
    "batch_size = 256\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "model = MLPModel()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=20e-6)\n",
    "\n",
    "num_epochs = 150\n",
    "train_mlp_model(train_loader, val_loader, model, criterion, optimizer, num_epochs=num_epochs, patience=patience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating BERT Embeddings: 100%|█████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [00:18<00:00, 26.75it/s]\n",
      "Generating Test Predictions: 100%|█████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 167.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-uncased_13layers_early_stopping_head_1.5backtail_Nov_17_20:42_0.3386967182159424.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_texts, _ = load_data('sample_submission.csv')\n",
    "test_embeddings = embedding_function(test_texts)\n",
    "\n",
    "\n",
    "test_data = TensorDataset(torch.tensor(test_embeddings))\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "model.eval()\n",
    "test_preds = []\n",
    "with torch.no_grad():\n",
    "    for inputs in tqdm(test_loader, desc=\"Generating Test Predictions\"):\n",
    "        inputs = inputs[0].to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        test_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "\n",
    "test_preds = [x[0] for x  in test_preds]\n",
    "\n",
    "\n",
    "\n",
    "now = datetime.datetime.now().strftime(\"%b_%d_%H:%M\")\n",
    "validation_score = run_on_validation(val_loader, model)\n",
    "model_name = f\"bert-base-uncased_13layers_early_stopping_head_1.5backtail_{now}_{validation_score}.csv\"\n",
    "\n",
    "print(model_name)\n",
    "        \n",
    "# Write predictions to the output CSV file\n",
    "output_df = pd.DataFrame({'text': test_texts, 'score': test_preds})\n",
    "output_df.to_csv(model_name, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
